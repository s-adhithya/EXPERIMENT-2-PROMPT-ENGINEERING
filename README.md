# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
1. Identify different prompt types (broad, basic, refined, few-shot).
2. Select test scenarios (creative writing, technical explanation, decision-making).
3. Apply each prompt type to the scenarios and record responses.
4. Evaluate outputs based on clarity, accuracy, depth, and relevance.
5. Compare and analyze results.
## Output
### Example Comparative Test
<img width="963" height="362" alt="image" src="https://github.com/user-attachments/assets/d845d5ee-5b98-4eac-9dbc-1596db574555" />

<img width="980" height="368" alt="image" src="https://github.com/user-attachments/assets/15d658f8-7553-4c8b-a3f5-c33395d21434" />
<img width="949" height="399" alt="image" src="https://github.com/user-attachments/assets/effde989-4422-4814-80a6-2cddd38201a3" />

### Comparative Analysis
<img width="950" height="252" alt="image" src="https://github.com/user-attachments/assets/d7d4767b-5777-4199-9dde-d24e6c555650" />

## Result
Refined and structured prompts produce clearer, more accurate, and context-aware responses compared to broad or basic prompts, proving that effective prompt design greatly improves output quality.
